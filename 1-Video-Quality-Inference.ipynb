{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbGBJegvBuXV"
   },
   "source": "# Assignment: Video Quality Inference\n\nTo this point in the class, you have learned various techniques for leading and analyzing packet captures of various types, generating features from those packet captures, and training and evaluating models using those features.\n\nIn this assignment, you will put all of this together, using a network traffic trace to train a model to automatically infer video quality of experience from a labeled traffic trace.\n\n## Setup: Clone the Data Repository\n\nBefore starting, you need to clone the course data repository to access the datasets:\n\n```bash\ngit clone https://github.com/noise-courses/data.git\n```\n\nThis repository contains the Netflix packet capture needed for this assignment in the `data/video-qoe/` directory.\n\n## Part 1: Warmup\n\nThe first part of this assignment builds directly on the hands-on activities but extends them slightly.\n\n### Extract Features from the Network Traffic\n\nLoad the `netflix.pcap` file from `data/video-qoe/netflix.pcap`, which is a packet trace that includes network traffic."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOPPsKpYB6PS"
   },
   "source": [
    "### Identifying the Service Type\n",
    "\n",
    "Use the DNS traffic to filter the packet trace for Netflix traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qFV6q2OCCsK"
   },
   "source": [
    "### Generate Statistics\n",
    "\n",
    "Generate statistics and features for the Netflix traffic flows. Use the `netml` library or any other technique that you choose to generate a set of features that you think would be good features for your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qFV6q2OCCsK"
   },
   "source": [
    "**Write a brief justification for the features that you have chosen.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBCP_2SBC2xj"
   },
   "source": [
    "### Inferring Segment downloads\n",
    "\n",
    "In addition to the features that you could generate using the `netml` library or similar, add to your feature vector a \"segment downloads rate\" feature, which indicates the number of video segments downloaded for a given time window.\n",
    "\n",
    "Note: If you are using the `netml` library, generating features with `SAMP` style options may be useful, as this option gives you time windows, and you can then simply add the segment download rate to that existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Part 2: Video Quality Inference\n\nYou will now load the complete video dataset from a previous study to train and test models based on these features to automatically infer the quality of a streaming video flow.\n\nFor this part of the assignment, you will need to decompress the video dataset file:\n\n```bash\n# Decompress the dataset (creates video_dataset.pkl, 265 MB)\nxz -d data/video-qoe/video_dataset.pkl.xz\n```\n\nThis dataset contains 204,713 samples with 170 features from Netflix, YouTube, Twitch, and Amazon Prime Video sessions."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the File\n",
    "\n",
    "Load the video dataset pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the File\n",
    "\n",
    "1. The dataset contains video resolutions that are not valid. Remove entries in the dataset that do not contain a valid video resolution. Valid resolutions are 240, 360, 480, 720, 1080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The file also contains columns that are unnecessary (in fact, unhelpful!) for performing predictions. Identify those columns, and remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly explain why you removed those columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Your Data\n",
    "\n",
    "Prepare your data matrix, determine your features and labels, and perform a train-test split on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Tune Your Model\n",
    "\n",
    "1. Select a model of your choice.\n",
    "2. Train the model using your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune Your Model\n",
    "\n",
    "Perform hyperparameter tuning to find optimal parameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Your Model\n",
    "\n",
    "Evaluate your model accuracy according to the following metrics:\n",
    "\n",
    "1. Accuracy\n",
    "2. F1 Score\n",
    "3. Confusion Matrix\n",
    "4. ROC/AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predict the Ongoing Resolution of a Real Netflix Session\n",
    "\n",
    "Now that you have your model, it's time to put it in practice!\n",
    "\n",
    "Use a preprocessed Netflix video session to infer **and plot** the resolution at 10-second time intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM0Cd1i3qtplCsVAB9qTjxA",
   "collapsed_sections": [],
   "name": "pcap_processing_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}